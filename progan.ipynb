{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antpc/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/antpc/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=1,gain=2):\n",
    "        super(WeightedSumConv2d,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        self.scale=(gain/(in_channels*(kernel_size**2)))**0.5\n",
    "        self.bias=self.conv.bias\n",
    "        self.conv.bias=None\n",
    "        # initalize convolution layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.conv(x*self.scale)+self.bias.view(1,self.bias.shape[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.eps=1e-8\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x/torch.sqrt(torch.mean(x**2,dim=1,keepdim=True)+self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,use_pix_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pix_norm=use_pix_norm\n",
    "        self.con1=WeightedSumConv2d(in_channels,out_channels)\n",
    "        self.con2=WeightedSumConv2d(out_channels,out_channels)\n",
    "        self.leaky=nn.LeakyReLU(0.2)\n",
    "        self.pn=PixelNorm()\n",
    "    def forward(self,x):\n",
    "        x=self.leaky(self.con1(x))\n",
    "        x=self.pn(x) if self.use_pix_norm else x\n",
    "        x=self.leaky(self.con2(x))\n",
    "        x=self.pn(x) if self.use_pix_norm else x\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,nosie_dim,in_channels,img_channels=3) -> None:\n",
    "        super().__init__()\n",
    "        self.inital_block=nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(nosie_dim,in_channels,4,1,0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedSumConv2d(in_channels,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "        \n",
    "        self.inital_rgb=WeightedSumConv2d(in_channels,img_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.progessive_blocks,self.rgb_layers=nn.ModuleList([]),nn.ModuleList([self.inital_rgb])\n",
    "        \n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c=int(in_channels*factors[i])\n",
    "            conv_out_c=int(in_channels*factors[i+1])\n",
    "            self.progessive_blocks.append(ConvBlock(conv_in_c,conv_out_c))\n",
    "            self.rgb_layers.append(WeightedSumConv2d(conv_out_c,img_channels,\n",
    "                                                     kernel_size=1,stride=1,padding=0))\n",
    "    def fade_in(self,alpha,upsclaed,generated):\n",
    "        return torch.tanh(alpha*generated+(1-alpha)*upsclaed)\n",
    "    \n",
    "    def forward(self,x,alpha,steps):\n",
    "        out=self.inital_block(x)\n",
    "        # if step=0 4x4 if step=1 8x8 ......\n",
    "        if steps==0:\n",
    "            return self.inital_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled=nn.functional.interpolate(out,scale_factor=2,mode='nearest')\n",
    "            out=self.progessive_blocks[step](upscaled)\n",
    "            \n",
    "        final_upscaled=self.rgb_layers[steps-1](upscaled)\n",
    "        final_out=self.rgb_layers[steps](out)\n",
    "        \n",
    "        return self.fade_in(alpha,final_upscaled,final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticDiscriminator(nn.Module):\n",
    "    def __init__(self,noise_dim,in_channels,img_channels) -> None:\n",
    "        super(CriticDiscriminator,self).__init__()\n",
    "        self._progessive_block,self.rgb_layers=nn.ModuleList([]),nn.ModuleList([])\n",
    "        self.leaky=nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors)-1,0,-1):\n",
    "            con_in_c=int(in_channels*factors[i])\n",
    "            con_in_out=int(in_channels*factors[i-1])\n",
    "            self._progessive_block.append(ConvBlock(con_in_c,con_in_out,use_pix_norm=False))\n",
    "            \n",
    "            self.rgb_layers.append(WeightedSumConv2d(img_channels,con_in_c,kernel_size=1,stride=1,padding=0))\n",
    "            \n",
    "        self.initial_rgb=WeightedSumConv2d(img_channels,in_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.avg_pool=nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.final_layer=nn.Sequential(\n",
    "            WeightedSumConv2d(in_channels+1,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedSumConv2d(in_channels,in_channels,kernel_size=4,stride=1,padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedSumConv2d(in_channels,1,kernel_size=1,stride=1,padding=0),\n",
    "        )\n",
    "    def fade_in(self,alpha,downscale,out):\n",
    "        return (alpha*out +(1-alpha)*downscale)\n",
    "    \n",
    "    def minibatchstd(self,x):\n",
    "        batch_statistics=torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3])\n",
    "        return torch.cat([x,batch_statistics],dim=1)\n",
    "    \n",
    "    def forward(self,x,alpha,steps):# steps=0 (4x4). step=1 (8x8)\n",
    "        current_step=len(self._progessive_block)-steps\n",
    "        out=self.leaky(self.rgb_layers[current_step](x))\n",
    "        \n",
    "        if steps==0:\n",
    "            out=self.minibatchstd(out)\n",
    "            return self.final_layer(out).view(out.shape[0],-1)\n",
    "        \n",
    "        downscaled=self.leaky(self.rgb_layers[current_step+1](self.avg_pool(x)))\n",
    "        out=self.avg_pool(self._progessive_block[current_step](out))\n",
    "        out=self.fade_in(alpha,downscaled,out)\n",
    "        \n",
    "        for step in range(current_step+1,len(self._progessive_block)):\n",
    "            out=self.avg_pool(self._progessive_block[step](out))\n",
    "        out=self.minibatchstd(out)\n",
    "        return self.final_layer(out).view(out.shape[0],-1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_dim=50\n",
    "# in_channels=256\n",
    "# g=Generator(noise_dim,in_channels=in_channels,img_channels=3)\n",
    "# d=CriticDiscriminator(noise_dim,in_channels=in_channels,img_channels=3)\n",
    "# for img_size in [4,8,16,32,64,128,256,512,1024]:\n",
    "#     num_steps=int(log2(img_size/4))\n",
    "#     x=torch.randn(1,noise_dim,1,1)\n",
    "#     z=g(x,0.5,steps=num_steps)\n",
    "#     print(z.shape)\n",
    "#     assert z.shape == (1, 3, img_size, img_size)\n",
    "#     out = d(z, alpha=0.5, steps=num_steps)\n",
    "#     print(out.shape)\n",
    "#     assert out.shape == (1, 1)\n",
    "#     print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake,alpha,train_step,device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_channels=3\n",
    "def data_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(img_channels)],\n",
    "                [0.5 for _ in range(img_channels)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = bs[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=\"/mnt/disk1/Gulshan/GAN/ProGAN/celeba_hq/train\", transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 70, 70, 70, 70, 70, 70, 70, 70]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Start_Train_Img_size=4\n",
    "device=\"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr=1e-3\n",
    "num_workers=4\n",
    "lambda_GP=10\n",
    "critic_iteration=1\n",
    "bs = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
    "img_channels=3\n",
    "noise_dim=256\n",
    "in_channels=256\n",
    "progessive_epochs=[70]*len(bs)\n",
    "fixed_noise=torch.randn(8,noise_dim,1,1).to(device)\n",
    "epoch_step = int(log2(Start_Train_Img_size / 4))\n",
    "progessive_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generater=Generator(noise_dim,in_channels,img_channels).to(device)\n",
    "discriminator=CriticDiscriminator(noise_dim,in_channels,img_channels).to(device)\n",
    "opt_gen = optim.Adam(generater.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "opt_critic = optim.Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(0.0, 0.99)\n",
    ")\n",
    "scaler_critic = torch.cuda.amp.GradScaler()\n",
    "scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f\"logs/gan1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480e5d2a878a4493ae028df1d8549fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f155cfba23d4c63b0aca3aefea5bf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train():\n",
    "    generater.train()\n",
    "    discriminator.train()\n",
    "    for num_epochs in progessive_epochs[epoch_step:]:\n",
    "        alpha=1e-5\n",
    "        loader,dataset=data_loader(4 * 2 ** epoch_step)\n",
    "        print(f\"Current image size: {4 * 2 ** epoch_step}\")\n",
    "        for epoch in tqdm(range(num_epochs),total=num_epochs):\n",
    "            for idx,(real,_) in enumerate(tqdm(loader)):\n",
    "                real=real.to(device)\n",
    "                current_batch_size=real.shape[0]\n",
    "                # train critic discriminator\n",
    "                noise=torch.randn(current_batch_size,noise_dim,1,1).to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    fake=generater(noise,alpha,epoch_step)\n",
    "                    critic_real=discriminator(real,alpha,epoch_step)\n",
    "                    critic_fake=discriminator(fake.detach(),alpha,epoch_step)\n",
    "                    gp=gradient_penalty(discriminator,real,fake,alpha,epoch_step,device)\n",
    "                    \n",
    "                    loss_critic=(\n",
    "                        -(torch.mean(critic_real)- torch.mean(critic_fake))\n",
    "                        + lambda_GP*gp\n",
    "                        +(0.001*torch.mean(critic_real**2)) #0.001 -drift to make critic go fdar away from zero\n",
    "                    )\n",
    "                opt_critic.zero_grad()\n",
    "                scaler_critic.scale(loss_critic).backward()\n",
    "                scaler_critic.step(opt_critic)\n",
    "                scaler_critic.update()\n",
    "                    \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    gen_fake=discriminator(fake,alpha,epoch_step)\n",
    "                    loss_gen=-torch.mean(gen_fake)\n",
    "                \n",
    "                opt_gen.zero_grad()\n",
    "                scaler_gen.scale(loss_gen).backward()\n",
    "                scaler_gen.step(opt_gen)\n",
    "                scaler_gen.update()    \n",
    "                \n",
    "                alpha+=current_batch_size/(len(dataset)*progessive_epochs[epoch_step]*0.5)\n",
    "                \n",
    "                if idx % 500 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        fixed_fake = generater(fixed_noise, alpha, epoch_step) * 0.5 + 0.5\n",
    "                        img_grid_real = torchvision.utils.make_grid(real[:8].detach(), normalize=True)\n",
    "                        img_grid_fake = torchvision.utils.make_grid(fixed_fake[:8].detach(), normalize=True)\n",
    "                        writer.add_image(\"Real\", img_grid_real, global_step=epoch)\n",
    "                        writer.add_image(\"Fake\", img_grid_fake, global_step=epoch)\n",
    "                        writer.add_scalar(\"Loss Critic\", loss_critic.item(), global_step=epoch)\n",
    "                        writer.add_scalar(\"Loss generator\", loss_gen.item(), global_step=epoch)\n",
    "train()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gulshan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
