{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel ) -> None:\n",
    "        super(Discriminator,self).__init__()\n",
    "        #input: (bs,channel,64,64)\n",
    "        self.discrimator=nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.block(out_channel,out_channel*2,kernel_size=4,stride=2,padding=1),\n",
    "            self.block(out_channel*2,out_channel*4,kernel_size=4,stride=2,padding=1),\n",
    "            self.block(out_channel*4,out_channel*8,kernel_size=4,stride=2,padding=1),\n",
    "            nn.Conv2d(out_channel*8,1,kernel_size=4,stride=2,padding=0),\n",
    "            # nn.Sigmoid() # no for wgan there ios sigmoid ativation\n",
    "        )\n",
    "    def block(self,in_channel,out_channel,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel,kernel_size,stride,padding,bias=False))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.discrimator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=Discriminator(3,1)\n",
    "x=torch.randn(1,3,64,64)\n",
    "d(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,noise_dim,in_channels,out_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator=nn.Sequential(\n",
    "            self.block(noise_dim,out_channels*16,kernel_size=4,stride=2,padding=0),\n",
    "            self.block(out_channels*16,out_channels*8,kernel_size=4,stride=2,padding=1),\n",
    "            self.block(out_channels*8,out_channels*4,kernel_size=4,stride=2,padding=1),\n",
    "            self.block(out_channels*4,out_channels*2,kernel_size=4,stride=2,padding=1),\n",
    "            nn.ConvTranspose2d(out_channels*2,in_channels,4,2,1),\n",
    "            nn.Tanh(), #[-1,1]\n",
    "        )\n",
    "    def block(self,in_channel,out_channel,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channel,out_channel,kernel_size,stride,padding),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d,nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 64, 64])\n",
      "disc model torch.Size([4, 1, 1, 1])\n",
      "torch.Size([4, 100, 1, 1])\n",
      "torch.Size([4, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    noise_dim=100\n",
    "    b,in_channel,h,w=4,1,64,64\n",
    "    x=torch.randn((b,in_channel,h,w))\n",
    "    print(x.shape)\n",
    "    disc=Discriminator(in_channel,8)\n",
    "    initalize_weight(disc)\n",
    "    assert disc(x).shape == (b,1,1,1)\n",
    "    dis_model=disc(x)\n",
    "    print('disc model',dis_model.shape)\n",
    "    gen=Generator(noise_dim,in_channel,8)\n",
    "    y=torch.randn((b,noise_dim,1,1))\n",
    "    print(y.shape)\n",
    "    gen_model=gen(y)\n",
    "    assert gen_model.shape== (b,in_channel,h,w)\n",
    "    print(gen_model.shape)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "CUDA=torch.cuda.is_available()\n",
    "# CUDA=False\n",
    "device = \"cuda:7\" if CUDA else \"cpu\"\n",
    "lr = 5e-5\n",
    "noise_dim = 100\n",
    "batch_size = 64\n",
    "in_channel=3 #1\n",
    "Image_size=64\n",
    "num_epochs=10\n",
    "out_channel_gen=64\n",
    "out_channel_disc=64\n",
    "critic_iteration=10\n",
    "weight_clip=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize((Image_size,Image_size)),\n",
    "    # transforms.Resize(Image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5 for _ in range(in_channel)],\n",
    "        [0.5 for _ in range(in_channel)],  \n",
    "    )\n",
    "])\n",
    "# dataset=datasets.MNIST(root=\"dataset/\",train=True,transform=transform,download=True)\n",
    "dataset=datasets.ImageFolder(root=\"/mnt/disk1/Gulshan/dataset/Dog_data\",transform=transform)\n",
    "loader=DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "generator=Generator(noise_dim,in_channel,out_channel_gen).to(device)\n",
    "discrmintor_critic=Discriminator(in_channel,out_channel_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initalize_weight(generator)\n",
    "initalize_weight(discrmintor_critic)\n",
    "optimizer_gen=optim.RMSprop(generator.parameters(),lr=lr,)\n",
    "optimizer_disc_critic=optim.RMSprop(discrmintor_critic.parameters(),lr=lr,)\n",
    "fixed_noise=torch.randn(32,noise_dim,1,1).to(device)\n",
    "writer_real=SummaryWriter(\"logs/real\")\n",
    "writer_fake=SummaryWriter(\"logs/fake\")\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ae67d46e8a48b098115f1e567cc36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911d5177473c48d7b7f68e793d47077a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Batch 0/133                       Loss D: -675.6105, loss G: 33.9164,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5641b58c446e4dd59a8b2bd51b1785fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Batch 0/133                       Loss D: -481830464.0000, loss G: -77085088.0000,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c16808b47f64e10af87954996228ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Batch 0/133                       Loss D: -10502194176.0000, loss G: 1048001088.0000,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a2bed49e848aeb39e55c31464a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Batch 0/133                       Loss D: -43280515072.0000, loss G: -16130102.0000,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572e7ab98e5e4f1b8f32530941c5bdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Batch 0/133                       Loss D: -49071243264.0000, loss G: -57459306496.0000,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25f831cc46140b49c795f639d61be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Batch 0/133                       Loss D: -80632135680.0000, loss G: -235837030400.0000,\n"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "discrmintor_critic.train()\n",
    "for epoch in tqdm(range(num_epochs),total=num_epochs):\n",
    "    \n",
    "    running_loss_gen=0\n",
    "    for batch_idx, (real, _) in tqdm(enumerate(loader)):\n",
    "        real=real.to(device)\n",
    "        for _ in range(critic_iteration):\n",
    "            noise=torch.randn((batch_size,noise_dim,1,1)).to(device)\n",
    "            fake=generator(noise)\n",
    "            ##loss for discrimi\n",
    "            disc_critic_real=discrmintor_critic(real).reshape(-1)\n",
    "            disc_critic_fake=discrmintor_critic(fake).reshape(-1)\n",
    "            \n",
    "            loss_dics_critic=-(torch.mean(disc_critic_real)-torch.mean(disc_critic_fake))\n",
    "            \n",
    "            optimizer_disc_critic.zero_grad()\n",
    "            loss_dics_critic.backward(retain_graph=True)\n",
    "            optimizer_disc_critic.step()\n",
    "        ## losss for generater\n",
    "        \n",
    "        gen_out=discrmintor_critic(fake).reshape(-1)\n",
    "        gen_loss=-(torch.mean(gen_out))\n",
    "        optimizer_gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        running_loss_gen+=gen_loss\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {loss_dics_critic:.4f}, loss G: {gen_loss:.4f},\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                writer_fake.add_scalar(\"Fake loss\",running_loss_gen,global_step=step)\n",
    "                step += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.14.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "E0507 16:16:48.917974 139790653249280 _internal.py:96] Error on request:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/serving.py\", line 362, in run_wsgi\n",
      "    execute(self.server.app)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/serving.py\", line 323, in execute\n",
      "    application_iter = app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 528, in __call__\n",
      "    return self._app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 569, in wrapper\n",
      "    return wsgi_app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/security_validator.py\", line 91, in __call__\n",
      "    return self._application(environ, start_response_proxy)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/path_prefix.py\", line 68, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/experiment_id.py\", line 73, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/empty_path_redirect.py\", line 43, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/client_feature_flags.py\", line 55, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/auth_context_middleware.py\", line 38, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 551, in _route_request\n",
      "    return self.exact_routes[clean_path](environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/wrappers/request.py\", line 189, in application\n",
      "    resp = f(*args[:-2] + (request,))\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/plugins/hparams/hparams_plugin.py\", line 122, in get_experiment_route\n",
      "    json_format.MessageToJson(\n",
      "TypeError: MessageToJson() got an unexpected keyword argument 'including_default_value_fields'\n",
      "E0507 16:16:50.067149 139790577747712 _internal.py:96] Error on request:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/serving.py\", line 362, in run_wsgi\n",
      "    execute(self.server.app)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/serving.py\", line 323, in execute\n",
      "    application_iter = app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 528, in __call__\n",
      "    return self._app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 569, in wrapper\n",
      "    return wsgi_app(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/security_validator.py\", line 91, in __call__\n",
      "    return self._application(environ, start_response_proxy)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/path_prefix.py\", line 68, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/experiment_id.py\", line 73, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/empty_path_redirect.py\", line 43, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/client_feature_flags.py\", line 55, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/auth_context_middleware.py\", line 38, in __call__\n",
      "    return self._application(environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 551, in _route_request\n",
      "    return self.exact_routes[clean_path](environ, start_response)\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/werkzeug/wrappers/request.py\", line 189, in application\n",
      "    resp = f(*args[:-2] + (request,))\n",
      "  File \"/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/tensorboard/plugins/hparams/hparams_plugin.py\", line 122, in get_experiment_route\n",
      "    json_format.MessageToJson(\n",
      "TypeError: MessageToJson() got an unexpected keyword argument 'including_default_value_fields'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=/mnt/disk1/Gulshan/GAN/WGAN/logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gulshan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
